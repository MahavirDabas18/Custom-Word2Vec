{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2b595f",
   "metadata": {},
   "source": [
    "<h1> Word2Vec- Wikipedia Sentences </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52355949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries/module\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1f5fd",
   "metadata": {},
   "source": [
    "<h2> Loading the Text </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15deb9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the text is in the form of a csv file in the same folder\n",
    "#it is a 1gb txt file of wikipedia sentences\n",
    "#each line is a sentence\n",
    "\n",
    "with open('wikisent2.txt') as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa430ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Abkhazia's break-away government stated that a plane crashed, and rejected the claim that it was shot down.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now text is a list of all sentences as a string\n",
    "text[50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4795a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7871825"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc626f",
   "metadata": {},
   "source": [
    "the list has about 78 lakhs sentences\n",
    "Due to limited computing power, i have randomly sampled 500 sentences for which we will train the w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7dbec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=random.sample(text,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c00e7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6caa51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The song has peaked at numbers two and one on the Billboard Hot Country Songs and Country Airplay charts, respectively.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934fe8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c59b46bd",
   "metadata": {},
   "source": [
    "<h2> Data Cleaning </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019d34a",
   "metadata": {},
   "source": [
    "<b>Text Preprocessing</b>\n",
    "Now that we have finished loading, our data requires some preprocessing before we go on and train the w2v model\n",
    "\n",
    "Hence in the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "1. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "2. Check if the word is made up of english letters and is not alpha-numeric\n",
    "3. Convert the word to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcd1e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to decontract as many words as possible\n",
    "def decontracted(sentence):\n",
    "    # specific\n",
    "    sentence = re.sub(r\"won\\'t\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n",
    "\n",
    "    # general\n",
    "    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d479d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_alpha_numeric(sentence): #words with numbers as well\n",
    "    sentence=re.sub(r\"\\S*\\d+\\S*\",\"\",sentence)\n",
    "    return sentence\n",
    "#zero or more non space followed by one or more digits followed by zero or more non space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00fbbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(sentence): # @ # $ % ^ & <  etc\n",
    "    sentence=re.sub(r\"[^a-zA-Z\\S]+\",\" \",sentence)\n",
    "    return sentence\n",
    "#not a-z or A-Z or white space- 1 or more such occurences-we will replace them with a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf27ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(sentence):\n",
    "    sentence=re.sub(r\"[^\\w\\s]\",\" \",sentence)\n",
    "    return sentence\n",
    "#not (word char or space char)- it has to be a punctuation if you have removed alpha numeric, html tags, url, and special char\n",
    "#therefore use it in the end- the function has been designed on this assumption that it will be used at last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3dcb3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_underscore(sentence):\n",
    "    sentence=re.sub(r\"_+\",\"\",sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd374d0",
   "metadata": {},
   "source": [
    "<h3> preprocessing/cleaning loop </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92a63c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 15688.20it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data=[] #list to store all the final preprocessed review texts\n",
    "\n",
    "for sentence in tqdm(text):\n",
    "    #.strip() removes all the trailing and leading characters (by default-white space) from a string\n",
    "    #using all the functions made for preprocessing\n",
    "    sentence=decontracted(sentence).strip()\n",
    "    sentence=remove_alpha_numeric(sentence).strip()\n",
    "    sentence=remove_special_char(sentence).strip()\n",
    "    sentence=remove_punctuations(sentence).strip() #used in the end\n",
    "    sentence=remove_underscore(sentence).strip()\n",
    "      \n",
    "    #removing stop words from the sentence \n",
    "    word_list=sentence.split() #splits the string by white spaces and stores it in a list\n",
    "    word_list=[i.lower() for i in word_list] #changing case of all words to lower case\n",
    "    word_list=[i for i in word_list if len(i)>1] #removing single characters\n",
    "    sentence=\" \".join(word_list) #sentence has been created from all the final words in word_list\n",
    "    sentence.strip()\n",
    "    preprocessed_data.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30247f3",
   "metadata": {},
   "source": [
    "now the data preprocessing is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910917cc",
   "metadata": {},
   "source": [
    "<b> comparing results before and after preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5853d680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Since early 2009, they have been on a silent hiatus.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c762c0d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'since early they have been on silent hiatus'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906d598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ccd3f24",
   "metadata": {},
   "source": [
    "<h2> Defining the Vocabulary </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb596dd",
   "metadata": {},
   "source": [
    "now we need a function to define the vocabulary of all unique words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e4222b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(data): #takes in a list of sentences\n",
    "    \n",
    "    unique_words = set() # at first we will initialize an empty set\n",
    "    for row in tqdm(data): # for each review in the list\n",
    "        for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
    "            unique_words.add(word)\n",
    "    #sorting the words in the list in alphabetical order\n",
    "    unique_words = sorted(list(unique_words))\n",
    "    #definning a dictionary giving each word a unique index\n",
    "    vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "    \n",
    "    return vocab #returning the vocab dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf5bede6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 125255.45it/s]\n"
     ]
    }
   ],
   "source": [
    "#getting the vocabulary for our case\n",
    "vocab=get_vocab(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f65edda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now our vocabulary has been defined and each word in preprocessed data has its own unique index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5800d18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abandoned': 0,\n",
       " 'abandonment': 1,\n",
       " 'abbandonata': 2,\n",
       " 'abbreviated': 3,\n",
       " 'abbreviation': 4,\n",
       " 'abco': 5,\n",
       " 'abdourahman': 6,\n",
       " 'abolition': 7,\n",
       " 'aboriginal': 8,\n",
       " 'about': 9,\n",
       " 'above': 10,\n",
       " 'abramowitz': 11,\n",
       " 'abyssinia': 12,\n",
       " 'academic': 13,\n",
       " 'acanthoscelides': 14,\n",
       " 'accepted': 15,\n",
       " 'accident': 16,\n",
       " 'accommodate': 17,\n",
       " 'accordance': 18,\n",
       " 'according': 19,\n",
       " 'accounts': 20,\n",
       " 'accredited': 21,\n",
       " 'ace': 22,\n",
       " 'achievement': 23,\n",
       " 'acquired': 24,\n",
       " 'acquisition': 25,\n",
       " 'acres': 26,\n",
       " 'across': 27,\n",
       " 'act': 28,\n",
       " 'action': 29,\n",
       " 'activated': 30,\n",
       " 'active': 31,\n",
       " 'activist': 32,\n",
       " 'activities': 33,\n",
       " 'activity': 34,\n",
       " 'actor': 35,\n",
       " 'actress': 36,\n",
       " 'acts': 37,\n",
       " 'actual': 38,\n",
       " 'ada': 39,\n",
       " 'adaptation': 40,\n",
       " 'adapted': 41,\n",
       " 'adc': 42,\n",
       " 'added': 43,\n",
       " 'addition': 44,\n",
       " 'adds': 45,\n",
       " 'adelaide': 46,\n",
       " 'adjacent': 47,\n",
       " 'administration': 48,\n",
       " 'adolph': 49,\n",
       " 'advanced': 50,\n",
       " 'advancing': 51,\n",
       " 'advocates': 52,\n",
       " 'adwa': 53,\n",
       " 'aerial': 54,\n",
       " 'aeronautica': 55,\n",
       " 'affected': 56,\n",
       " 'africa': 57,\n",
       " 'african': 58,\n",
       " 'after': 59,\n",
       " 'aga': 60,\n",
       " 'again': 61,\n",
       " 'against': 62,\n",
       " 'age': 63,\n",
       " 'aged': 64,\n",
       " 'agency': 65,\n",
       " 'agenda': 66,\n",
       " 'agglomeration': 67,\n",
       " 'agonis': 68,\n",
       " 'agricultural': 69,\n",
       " 'ahoms': 70,\n",
       " 'ailey': 71,\n",
       " 'aims': 72,\n",
       " 'air': 73,\n",
       " 'aircraft': 74,\n",
       " 'aired': 75,\n",
       " 'aires': 76,\n",
       " 'airing': 77,\n",
       " 'airlines': 78,\n",
       " 'airplay': 79,\n",
       " 'airport': 80,\n",
       " 'airways': 81,\n",
       " 'alabama': 82,\n",
       " 'alan': 83,\n",
       " 'alberta': 84,\n",
       " 'albion': 85,\n",
       " 'album': 86,\n",
       " 'albums': 87,\n",
       " 'alcohol': 88,\n",
       " 'alert': 89,\n",
       " 'algaida': 90,\n",
       " 'all': 91,\n",
       " 'allegheny': 92,\n",
       " 'allen': 93,\n",
       " 'allowed': 94,\n",
       " 'aloha': 95,\n",
       " 'alone': 96,\n",
       " 'along': 97,\n",
       " 'alongside': 98,\n",
       " 'aloysius': 99,\n",
       " 'also': 100,\n",
       " 'alternative': 101,\n",
       " 'although': 102,\n",
       " 'altogether': 103,\n",
       " 'alvin': 104,\n",
       " 'amalgamation': 105,\n",
       " 'amanda': 106,\n",
       " 'amateur': 107,\n",
       " 'amaurochalinus': 108,\n",
       " 'ambridge': 109,\n",
       " 'amenities': 110,\n",
       " 'america': 111,\n",
       " 'american': 112,\n",
       " 'americans': 113,\n",
       " 'amethi': 114,\n",
       " 'among': 115,\n",
       " 'amongst': 116,\n",
       " 'amounting': 117,\n",
       " 'amzi': 118,\n",
       " 'an': 119,\n",
       " 'analgesic': 120,\n",
       " 'analog': 121,\n",
       " 'analysis': 122,\n",
       " 'anatomic': 123,\n",
       " 'anchor': 124,\n",
       " 'and': 125,\n",
       " 'andes': 126,\n",
       " 'andhra': 127,\n",
       " 'andover': 128,\n",
       " 'andrusyshyn': 129,\n",
       " 'andy': 130,\n",
       " 'angeles': 131,\n",
       " 'angles': 132,\n",
       " 'animal': 133,\n",
       " 'anne': 134,\n",
       " 'annika': 135,\n",
       " 'announced': 136,\n",
       " 'annual': 137,\n",
       " 'annular': 138,\n",
       " 'another': 139,\n",
       " 'anoxic': 140,\n",
       " 'anseriformes': 141,\n",
       " 'anti': 142,\n",
       " 'antonio': 143,\n",
       " 'antv': 144,\n",
       " 'anupam': 145,\n",
       " 'anuradha': 146,\n",
       " 'any': 147,\n",
       " 'anyone': 148,\n",
       " 'apis': 149,\n",
       " 'apollo': 150,\n",
       " 'appeal': 151,\n",
       " 'appear': 152,\n",
       " 'appearance': 153,\n",
       " 'appearances': 154,\n",
       " 'appeared': 155,\n",
       " 'appears': 156,\n",
       " 'apple': 157,\n",
       " 'appliances': 158,\n",
       " 'applicable': 159,\n",
       " 'application': 160,\n",
       " 'applied': 161,\n",
       " 'appointed': 162,\n",
       " 'apprentices': 163,\n",
       " 'approximately': 164,\n",
       " 'april': 165,\n",
       " 'aqdas': 166,\n",
       " 'aquin': 167,\n",
       " 'arboretum': 168,\n",
       " 'architect': 169,\n",
       " 'architectural': 170,\n",
       " 'architecture': 171,\n",
       " 'ardisia': 172,\n",
       " 'are': 173,\n",
       " 'area': 174,\n",
       " 'areas': 175,\n",
       " 'arena': 176,\n",
       " 'argentina': 177,\n",
       " 'argued': 178,\n",
       " 'aribert': 179,\n",
       " 'arizona': 180,\n",
       " 'armed': 181,\n",
       " 'arms': 182,\n",
       " 'arnold': 183,\n",
       " 'arose': 184,\n",
       " 'around': 185,\n",
       " 'arrangement': 186,\n",
       " 'arrangements': 187,\n",
       " 'arriving': 188,\n",
       " 'arrondissement': 189,\n",
       " 'art': 190,\n",
       " 'article': 191,\n",
       " 'artist': 192,\n",
       " 'artistic': 193,\n",
       " 'artists': 194,\n",
       " 'arts': 195,\n",
       " 'artwork': 196,\n",
       " 'arundel': 197,\n",
       " 'arunta': 198,\n",
       " 'as': 199,\n",
       " 'asia': 200,\n",
       " 'asian': 201,\n",
       " 'aside': 202,\n",
       " 'asimov': 203,\n",
       " 'asind': 204,\n",
       " 'aspects': 205,\n",
       " 'assembly': 206,\n",
       " 'assessed': 207,\n",
       " 'assigned': 208,\n",
       " 'associated': 209,\n",
       " 'association': 210,\n",
       " 'asteroid': 211,\n",
       " 'astrolog': 212,\n",
       " 'astrology': 213,\n",
       " 'at': 214,\n",
       " 'athens': 215,\n",
       " 'athlete': 216,\n",
       " 'atlantic': 217,\n",
       " 'atmospheric': 218,\n",
       " 'atoms': 219,\n",
       " 'atroviolacea': 220,\n",
       " 'attack': 221,\n",
       " 'attacks': 222,\n",
       " 'attempt': 223,\n",
       " 'attempted': 224,\n",
       " 'attendance': 225,\n",
       " 'attention': 226,\n",
       " 'attribute': 227,\n",
       " 'auckland': 228,\n",
       " 'audio': 229,\n",
       " 'audrain': 230,\n",
       " 'august': 231,\n",
       " 'australasia': 232,\n",
       " 'australia': 233,\n",
       " 'australian': 234,\n",
       " 'austronesian': 235,\n",
       " 'author': 236,\n",
       " 'authorities': 237,\n",
       " 'authority': 238,\n",
       " 'autobiographical': 239,\n",
       " 'automation': 240,\n",
       " 'automobile': 241,\n",
       " 'available': 242,\n",
       " 'average': 243,\n",
       " 'aviation': 244,\n",
       " 'awa': 245,\n",
       " 'award': 246,\n",
       " 'awarded': 247,\n",
       " 'awards': 248,\n",
       " 'ay': 249,\n",
       " 'baby': 250,\n",
       " 'back': 251,\n",
       " 'backbone': 252,\n",
       " 'backgrounds': 253,\n",
       " 'backpackers': 254,\n",
       " 'backup': 255,\n",
       " 'bacteria': 256,\n",
       " 'baerti': 257,\n",
       " 'bag': 258,\n",
       " 'baggio': 259,\n",
       " 'bainter': 260,\n",
       " 'balan': 261,\n",
       " 'ballingrud': 262,\n",
       " 'balls': 263,\n",
       " 'balmain': 264,\n",
       " 'baloch': 265,\n",
       " 'balochistan': 266,\n",
       " 'baltimore': 267,\n",
       " 'bamboo': 268,\n",
       " 'bambusa': 269,\n",
       " 'band': 270,\n",
       " 'bands': 271,\n",
       " 'bandy': 272,\n",
       " 'bar': 273,\n",
       " 'barber': 274,\n",
       " 'barnaby': 275,\n",
       " 'barnes': 276,\n",
       " 'baseball': 277,\n",
       " 'based': 278,\n",
       " 'basis': 279,\n",
       " 'basketball': 280,\n",
       " 'bates': 281,\n",
       " 'battalion': 282,\n",
       " 'battery': 283,\n",
       " 'battle': 284,\n",
       " 'baumgartner': 285,\n",
       " 'bbc': 286,\n",
       " 'bc': 287,\n",
       " 'bce': 288,\n",
       " 'be': 289,\n",
       " 'beach': 290,\n",
       " 'beacon': 291,\n",
       " 'beat': 292,\n",
       " 'beaten': 293,\n",
       " 'beating': 294,\n",
       " 'beatrice': 295,\n",
       " 'beautiful': 296,\n",
       " 'became': 297,\n",
       " 'become': 298,\n",
       " 'becomes': 299,\n",
       " 'been': 300,\n",
       " 'beetle': 301,\n",
       " 'before': 302,\n",
       " 'befriended': 303,\n",
       " 'began': 304,\n",
       " 'beginning': 305,\n",
       " 'behalf': 306,\n",
       " 'behavior': 307,\n",
       " 'behnoosh': 308,\n",
       " 'beijing': 309,\n",
       " 'being': 310,\n",
       " 'believed': 311,\n",
       " 'believing': 312,\n",
       " 'bell': 313,\n",
       " 'bellied': 314,\n",
       " 'bellshill': 315,\n",
       " 'belonging': 316,\n",
       " 'belongs': 317,\n",
       " 'belt': 318,\n",
       " 'bench': 319,\n",
       " 'beneficial': 320,\n",
       " 'benefits': 321,\n",
       " 'benjamin': 322,\n",
       " 'benton': 323,\n",
       " 'benue': 324,\n",
       " 'bergen': 325,\n",
       " 'bernadette': 326,\n",
       " 'bertalanffy': 327,\n",
       " 'beru': 328,\n",
       " 'best': 329,\n",
       " 'better': 330,\n",
       " 'between': 331,\n",
       " 'bheeman': 332,\n",
       " 'bhilwara': 333,\n",
       " 'bidens': 334,\n",
       " 'biggest': 335,\n",
       " 'bihar': 336,\n",
       " 'bilingual': 337,\n",
       " 'bill': 338,\n",
       " 'billboard': 339,\n",
       " 'billboards': 340,\n",
       " 'billericay': 341,\n",
       " 'billion': 342,\n",
       " 'billy': 343,\n",
       " 'binary': 344,\n",
       " 'biologically': 345,\n",
       " 'bird': 346,\n",
       " 'birds': 347,\n",
       " 'birthday': 348,\n",
       " 'bisges': 349,\n",
       " 'bishop': 350,\n",
       " 'bite': 351,\n",
       " 'bixby': 352,\n",
       " 'black': 353,\n",
       " 'blackwell': 354,\n",
       " 'blake': 355,\n",
       " 'blancs': 356,\n",
       " 'blanket': 357,\n",
       " 'blastosporella': 358,\n",
       " 'block': 359,\n",
       " 'blogger': 360,\n",
       " 'blood': 361,\n",
       " 'blu': 362,\n",
       " 'blue': 363,\n",
       " 'blues': 364,\n",
       " 'bn': 365,\n",
       " 'board': 366,\n",
       " 'boat': 367,\n",
       " 'bob': 368,\n",
       " 'body': 369,\n",
       " 'boiler': 370,\n",
       " 'bold': 371,\n",
       " 'bond': 372,\n",
       " 'bonus': 373,\n",
       " 'book': 374,\n",
       " 'boosting': 375,\n",
       " 'border': 376,\n",
       " 'borders': 377,\n",
       " 'borland': 378,\n",
       " 'born': 379,\n",
       " 'borystheninae': 380,\n",
       " 'boston': 381,\n",
       " 'both': 382,\n",
       " 'bothriocerinae': 383,\n",
       " 'bottom': 384,\n",
       " 'bound': 385,\n",
       " 'bowbazar': 386,\n",
       " 'box': 387,\n",
       " 'boyhood': 388,\n",
       " 'braddock': 389,\n",
       " 'bradley': 390,\n",
       " 'brahmanical': 391,\n",
       " 'brand': 392,\n",
       " 'brands': 393,\n",
       " 'brian': 394,\n",
       " 'brick': 395,\n",
       " 'bridge': 396,\n",
       " 'bridges': 397,\n",
       " 'britain': 398,\n",
       " 'british': 399,\n",
       " 'broadcast': 400,\n",
       " 'broadway': 401,\n",
       " 'bronze': 402,\n",
       " 'brooklyn': 403,\n",
       " 'brosnan': 404,\n",
       " 'broubster': 405,\n",
       " 'brought': 406,\n",
       " 'brown': 407,\n",
       " 'brunei': 408,\n",
       " 'brunswick': 409,\n",
       " 'buenos': 410,\n",
       " 'buffy': 411,\n",
       " 'building': 412,\n",
       " 'buildings': 413,\n",
       " 'built': 414,\n",
       " 'bunchball': 415,\n",
       " 'burial': 416,\n",
       " 'burkholderiaceae': 417,\n",
       " 'burn': 418,\n",
       " 'burren': 419,\n",
       " 'bus': 420,\n",
       " 'bush': 421,\n",
       " 'business': 422,\n",
       " 'businessman': 423,\n",
       " 'but': 424,\n",
       " 'by': 425,\n",
       " 'cabinets': 426,\n",
       " 'cable': 427,\n",
       " 'cadbury': 428,\n",
       " 'cairn': 429,\n",
       " 'calcutta': 430,\n",
       " 'calgary': 431,\n",
       " 'california': 432,\n",
       " 'call': 433,\n",
       " 'called': 434,\n",
       " 'callionymus': 435,\n",
       " 'came': 436,\n",
       " 'cameo': 437,\n",
       " 'cameroon': 438,\n",
       " 'camp': 439,\n",
       " 'campbell': 440,\n",
       " 'campus': 441,\n",
       " 'campuses': 442,\n",
       " 'can': 443,\n",
       " 'canada': 444,\n",
       " 'canadian': 445,\n",
       " 'candidate': 446,\n",
       " 'canning': 447,\n",
       " 'canowindra': 448,\n",
       " 'cap': 449,\n",
       " 'capacities': 450,\n",
       " 'capacity': 451,\n",
       " 'capital': 452,\n",
       " 'captured': 453,\n",
       " 'car': 454,\n",
       " 'carbon': 455,\n",
       " 'carceri': 456,\n",
       " 'card': 457,\n",
       " 'career': 458,\n",
       " 'cargo': 459,\n",
       " 'carinata': 460,\n",
       " 'carleton': 461,\n",
       " 'carlos': 462,\n",
       " 'carolina': 463,\n",
       " 'carpenter': 464,\n",
       " 'carrie': 465,\n",
       " 'carried': 466,\n",
       " 'cartesian': 467,\n",
       " 'cartoon': 468,\n",
       " 'cartoons': 469,\n",
       " 'case': 470,\n",
       " 'cast': 471,\n",
       " 'castle': 472,\n",
       " 'castrato': 473,\n",
       " 'castrucci': 474,\n",
       " 'catalog': 475,\n",
       " 'catalogs': 476,\n",
       " 'catalysis': 477,\n",
       " 'caused': 478,\n",
       " 'cavaletto': 479,\n",
       " 'cavalry': 480,\n",
       " 'cave': 481,\n",
       " 'cbeebies': 482,\n",
       " 'cd': 483,\n",
       " 'cdp': 484,\n",
       " 'ceased': 485,\n",
       " 'ceases': 486,\n",
       " 'celebrities': 487,\n",
       " 'cell': 488,\n",
       " 'cells': 489,\n",
       " 'cennamo': 490,\n",
       " 'cenni': 491,\n",
       " 'census': 492,\n",
       " 'center': 493,\n",
       " 'central': 494,\n",
       " 'centring': 495,\n",
       " 'century': 496,\n",
       " 'ceramic': 497,\n",
       " 'chad': 498,\n",
       " 'chadurah': 499,\n",
       " 'chairman': 500,\n",
       " 'champion': 501,\n",
       " 'championship': 502,\n",
       " 'championships': 503,\n",
       " 'chandra': 504,\n",
       " 'change': 505,\n",
       " 'channel': 506,\n",
       " 'channels': 507,\n",
       " 'chapin': 508,\n",
       " 'chapman': 509,\n",
       " 'character': 510,\n",
       " 'characters': 511,\n",
       " 'charge': 512,\n",
       " 'charitable': 513,\n",
       " 'charles': 514,\n",
       " 'charleston': 515,\n",
       " 'charm': 516,\n",
       " 'chart': 517,\n",
       " 'charted': 518,\n",
       " 'charter': 519,\n",
       " 'charting': 520,\n",
       " 'charts': 521,\n",
       " 'chase': 522,\n",
       " 'check': 523,\n",
       " 'checks': 524,\n",
       " 'cheema': 525,\n",
       " 'chemical': 526,\n",
       " 'chennai': 527,\n",
       " 'chicago': 528,\n",
       " 'chiefly': 529,\n",
       " 'child': 530,\n",
       " 'children': 531,\n",
       " 'chilean': 532,\n",
       " 'china': 533,\n",
       " 'chinese': 534,\n",
       " 'cholapuram': 535,\n",
       " 'cholet': 536,\n",
       " 'choletais': 537,\n",
       " 'choreographed': 538,\n",
       " 'chosen': 539,\n",
       " 'christinae': 540,\n",
       " 'chrysomelidae': 541,\n",
       " 'chrysosporium': 542,\n",
       " 'churchyard': 543,\n",
       " 'cincinnati': 544,\n",
       " 'cine': 545,\n",
       " 'cinema': 546,\n",
       " 'cinematics': 547,\n",
       " 'cisterns': 548,\n",
       " 'cite': 549,\n",
       " 'cities': 550,\n",
       " 'citizens': 551,\n",
       " 'city': 552,\n",
       " 'civic': 553,\n",
       " 'civil': 554,\n",
       " 'civilians': 555,\n",
       " 'civitas': 556,\n",
       " 'clade': 557,\n",
       " 'clan': 558,\n",
       " 'clare': 559,\n",
       " 'claremont': 560,\n",
       " 'clarifications': 561,\n",
       " 'class': 562,\n",
       " 'classic': 563,\n",
       " 'claus': 564,\n",
       " 'cleaning': 565,\n",
       " 'clemson': 566,\n",
       " 'client': 567,\n",
       " 'cliff': 568,\n",
       " 'climate': 569,\n",
       " 'clinical': 570,\n",
       " 'close': 571,\n",
       " 'closed': 572,\n",
       " 'closedown': 573,\n",
       " 'closely': 574,\n",
       " 'closure': 575,\n",
       " 'cloth': 576,\n",
       " 'club': 577,\n",
       " 'clubs': 578,\n",
       " 'cluster': 579,\n",
       " 'cm': 580,\n",
       " 'cnbc': 581,\n",
       " 'co': 582,\n",
       " 'coach': 583,\n",
       " 'coached': 584,\n",
       " 'coal': 585,\n",
       " 'coalitions': 586,\n",
       " 'coat': 587,\n",
       " 'cochrane': 588,\n",
       " 'code': 589,\n",
       " 'coffee': 590,\n",
       " 'coital': 591,\n",
       " 'coking': 592,\n",
       " 'cole': 593,\n",
       " 'collaborator': 594,\n",
       " 'collapsed': 595,\n",
       " 'collection': 596,\n",
       " 'collective': 597,\n",
       " 'collectively': 598,\n",
       " 'college': 599,\n",
       " 'colonial': 600,\n",
       " 'colony': 601,\n",
       " 'columbia': 602,\n",
       " 'combination': 603,\n",
       " 'combines': 604,\n",
       " 'combustion': 605,\n",
       " 'come': 606,\n",
       " 'comedy': 607,\n",
       " 'comes': 608,\n",
       " 'comfortable': 609,\n",
       " 'comics': 610,\n",
       " 'commemoration': 611,\n",
       " 'commendatore': 612,\n",
       " 'commerce': 613,\n",
       " 'commercial': 614,\n",
       " 'commission': 615,\n",
       " 'commissioned': 616,\n",
       " 'common': 617,\n",
       " 'commonly': 618,\n",
       " 'commons': 619,\n",
       " 'commonwealth': 620,\n",
       " 'communal': 621,\n",
       " 'communities': 622,\n",
       " 'community': 623,\n",
       " 'commuter': 624,\n",
       " 'companies': 625,\n",
       " 'company': 626,\n",
       " 'compared': 627,\n",
       " 'compete': 628,\n",
       " 'competed': 629,\n",
       " 'competition': 630,\n",
       " 'compilation': 631,\n",
       " 'compiled': 632,\n",
       " 'completed': 633,\n",
       " 'completely': 634,\n",
       " 'complex': 635,\n",
       " 'composite': 636,\n",
       " 'comprises': 637,\n",
       " 'conakry': 638,\n",
       " 'concept': 639,\n",
       " 'concern': 640,\n",
       " 'concert': 641,\n",
       " 'concerts': 642,\n",
       " 'conducted': 643,\n",
       " 'conductor': 644,\n",
       " 'conducts': 645,\n",
       " 'cone': 646,\n",
       " 'confederate': 647,\n",
       " 'congo': 648,\n",
       " 'congress': 649,\n",
       " 'congressional': 650,\n",
       " 'conidiobolus': 651,\n",
       " 'coniferous': 652,\n",
       " 'connection': 653,\n",
       " 'consciousness': 654,\n",
       " 'consecutive': 655,\n",
       " 'consequence': 656,\n",
       " 'conservation': 657,\n",
       " 'consists': 658,\n",
       " 'consolidating': 659,\n",
       " 'constituency': 660,\n",
       " 'constraints': 661,\n",
       " 'consultancy': 662,\n",
       " 'consultant': 663,\n",
       " 'consumers': 664,\n",
       " 'contact': 665,\n",
       " 'contain': 666,\n",
       " 'contains': 667,\n",
       " 'contaminated': 668,\n",
       " 'conte': 669,\n",
       " 'contemporary': 670,\n",
       " 'content': 671,\n",
       " 'contested': 672,\n",
       " 'contests': 673,\n",
       " 'continuation': 674,\n",
       " 'continue': 675,\n",
       " 'continues': 676,\n",
       " 'contributing': 677,\n",
       " 'controversy': 678,\n",
       " 'convention': 679,\n",
       " 'conventions': 680,\n",
       " 'conversion': 681,\n",
       " 'conway': 682,\n",
       " 'cooper': 683,\n",
       " 'cooperated': 684,\n",
       " 'coordinating': 685,\n",
       " 'coordinator': 686,\n",
       " 'coppa': 687,\n",
       " 'coral': 688,\n",
       " 'corbett': 689,\n",
       " 'coronatus': 690,\n",
       " 'corporate': 691,\n",
       " 'corporation': 692,\n",
       " 'cosmetic': 693,\n",
       " 'cost': 694,\n",
       " 'costs': 695,\n",
       " 'could': 696,\n",
       " 'council': 697,\n",
       " 'counsel': 698,\n",
       " 'countess': 699,\n",
       " 'counties': 700,\n",
       " 'country': 701,\n",
       " 'county': 702,\n",
       " 'couple': 703,\n",
       " 'courses': 704,\n",
       " 'court': 705,\n",
       " 'coventry': 706,\n",
       " 'cover': 707,\n",
       " 'coverage': 708,\n",
       " 'cowra': 709,\n",
       " 'craig': 710,\n",
       " 'crawl': 711,\n",
       " 'creamy': 712,\n",
       " 'create': 713,\n",
       " 'created': 714,\n",
       " 'creative': 715,\n",
       " 'credited': 716,\n",
       " 'credits': 717,\n",
       " 'creek': 718,\n",
       " 'crews': 719,\n",
       " 'cricketer': 720,\n",
       " 'crippled': 721,\n",
       " 'critics': 722,\n",
       " 'croatian': 723,\n",
       " 'cross': 724,\n",
       " 'crowned': 725,\n",
       " 'cruise': 726,\n",
       " 'cruises': 727,\n",
       " 'crushed': 728,\n",
       " 'cuba': 729,\n",
       " 'cumulative': 730,\n",
       " 'cup': 731,\n",
       " 'cupressus': 732,\n",
       " 'cups': 733,\n",
       " 'cura': 734,\n",
       " 'currently': 735,\n",
       " 'customers': 736,\n",
       " 'cut': 737,\n",
       " 'cutler': 738,\n",
       " 'cyclecar': 739,\n",
       " 'cysticercosis': 740,\n",
       " 'dahlia': 741,\n",
       " 'dakota': 742,\n",
       " 'dalle': 743,\n",
       " 'dam': 744,\n",
       " 'damselfly': 745,\n",
       " 'dan': 746,\n",
       " 'dandaragan': 747,\n",
       " 'dania': 748,\n",
       " 'daniel': 749,\n",
       " 'danish': 750,\n",
       " 'darius': 751,\n",
       " 'dark': 752,\n",
       " 'darka': 753,\n",
       " 'darren': 754,\n",
       " 'date': 755,\n",
       " 'dated': 756,\n",
       " 'dates': 757,\n",
       " 'daughter': 758,\n",
       " 'david': 759,\n",
       " 'davies': 760,\n",
       " 'davis': 761,\n",
       " 'dawn': 762,\n",
       " 'day': 763,\n",
       " 'dayal': 764,\n",
       " 'days': 765,\n",
       " 'daytime': 766,\n",
       " 'dbss': 767,\n",
       " 'dc': 768,\n",
       " 'de': 769,\n",
       " 'dead': 770,\n",
       " 'deal': 771,\n",
       " 'dean': 772,\n",
       " 'death': 773,\n",
       " 'debut': 774,\n",
       " 'debuted': 775,\n",
       " 'decatur': 776,\n",
       " 'december': 777,\n",
       " 'decided': 778,\n",
       " 'decider': 779,\n",
       " 'decker': 780,\n",
       " 'deco': 781,\n",
       " 'decrease': 782,\n",
       " 'dedicated': 783,\n",
       " 'defeated': 784,\n",
       " 'defenceman': 785,\n",
       " 'defense': 786,\n",
       " 'defensive': 787,\n",
       " 'defensively': 788,\n",
       " 'deficit': 789,\n",
       " 'defined': 790,\n",
       " 'defunct': 791,\n",
       " 'delay': 792,\n",
       " 'delayed': 793,\n",
       " 'delegates': 794,\n",
       " 'delicata': 795,\n",
       " 'delimitation': 796,\n",
       " 'delivering': 797,\n",
       " 'delta': 798,\n",
       " 'dementia': 799,\n",
       " 'democrat': 800,\n",
       " 'democratic': 801,\n",
       " 'democrats': 802,\n",
       " 'demographic': 803,\n",
       " 'demolished': 804,\n",
       " 'dennis': 805,\n",
       " 'denny': 806,\n",
       " 'denton': 807,\n",
       " 'department': 808,\n",
       " 'depend': 809,\n",
       " 'depicted': 810,\n",
       " 'depicts': 811,\n",
       " 'deployments': 812,\n",
       " 'depot': 813,\n",
       " 'derelict': 814,\n",
       " 'derives': 815,\n",
       " 'derogatory': 816,\n",
       " 'derrick': 817,\n",
       " 'des': 818,\n",
       " 'descent': 819,\n",
       " 'describe': 820,\n",
       " 'described': 821,\n",
       " 'describes': 822,\n",
       " 'desecration': 823,\n",
       " 'desenfans': 824,\n",
       " 'desert': 825,\n",
       " 'designated': 826,\n",
       " 'designed': 827,\n",
       " 'designing': 828,\n",
       " 'desired': 829,\n",
       " 'desmond': 830,\n",
       " 'despite': 831,\n",
       " 'destinations': 832,\n",
       " 'destroyer': 833,\n",
       " 'detain': 834,\n",
       " 'devaasuram': 835,\n",
       " 'develop': 836,\n",
       " 'developed': 837,\n",
       " 'developer': 838,\n",
       " 'development': 839,\n",
       " 'developmentally': 840,\n",
       " 'developments': 841,\n",
       " 'device': 842,\n",
       " 'devon': 843,\n",
       " 'dialect': 844,\n",
       " 'diaspora': 845,\n",
       " 'did': 846,\n",
       " 'didone': 847,\n",
       " 'diehl': 848,\n",
       " 'differed': 849,\n",
       " 'different': 850,\n",
       " 'differs': 851,\n",
       " 'dig': 852,\n",
       " 'dilipkumar': 853,\n",
       " 'dineen': 854,\n",
       " 'dioptis': 855,\n",
       " 'directed': 856,\n",
       " 'directorial': 857,\n",
       " 'directors': 858,\n",
       " 'directorship': 859,\n",
       " 'disciple': 860,\n",
       " 'discontinued': 861,\n",
       " 'discovering': 862,\n",
       " 'discovery': 863,\n",
       " 'disease': 864,\n",
       " 'disfellowship': 865,\n",
       " 'disney': 866,\n",
       " 'disorder': 867,\n",
       " 'displays': 868,\n",
       " 'dissatisfied': 869,\n",
       " 'dissenter': 870,\n",
       " 'dissolved': 871,\n",
       " 'distinct': 872,\n",
       " 'distinctive': 873,\n",
       " 'distributions': 874,\n",
       " 'district': 875,\n",
       " 'districts': 876,\n",
       " 'disused': 877,\n",
       " 'diverse': 878,\n",
       " 'divide': 879,\n",
       " 'division': 880,\n",
       " 'djibouti': 881,\n",
       " 'djiboutian': 882,\n",
       " 'dmitry': 883,\n",
       " 'doctors': 884,\n",
       " 'document': 885,\n",
       " 'documentary': 886,\n",
       " 'does': 887,\n",
       " 'dog': 888,\n",
       " 'dole': 889,\n",
       " 'domestic': 890,\n",
       " 'domina': 891,\n",
       " 'dominance': 892,\n",
       " 'dominant': 893,\n",
       " 'don': 894,\n",
       " 'donkey': 895,\n",
       " 'dorothy': 896,\n",
       " 'doud': 897,\n",
       " 'down': 898,\n",
       " 'downtown': 899,\n",
       " 'dpcm': 900,\n",
       " 'dr': 901,\n",
       " 'draft': 902,\n",
       " 'drag': 903,\n",
       " 'dragon': 904,\n",
       " 'dragonets': 905,\n",
       " 'dragoons': 906,\n",
       " 'draw': 907,\n",
       " 'drawn': 908,\n",
       " 'dream': 909,\n",
       " 'dresden': 910,\n",
       " 'drew': 911,\n",
       " 'driggs': 912,\n",
       " 'drinking': 913,\n",
       " 'dropped': 914,\n",
       " 'drummer': 915,\n",
       " 'dry': 916,\n",
       " 'du': 917,\n",
       " 'dubbed': 918,\n",
       " 'duck': 919,\n",
       " 'due': 920,\n",
       " 'dulieu': 921,\n",
       " 'duos': 922,\n",
       " 'durham': 923,\n",
       " 'during': 924,\n",
       " 'dutch': 925,\n",
       " 'duties': 926,\n",
       " 'dvd': 927,\n",
       " 'dwindling': 928,\n",
       " 'dwyer': 929,\n",
       " 'each': 930,\n",
       " 'eagle': 931,\n",
       " 'earl': 932,\n",
       " 'earlier': 933,\n",
       " 'early': 934,\n",
       " 'earned': 935,\n",
       " 'earth': 936,\n",
       " 'east': 937,\n",
       " 'eastern': 938,\n",
       " 'eastview': 939,\n",
       " 'eaton': 940,\n",
       " 'eclipse': 941,\n",
       " 'economics': 942,\n",
       " 'economy': 943,\n",
       " 'ed': 944,\n",
       " 'edge': 945,\n",
       " 'edgewood': 946,\n",
       " 'edinburgh': 947,\n",
       " 'edition': 948,\n",
       " 'editor': 949,\n",
       " 'eds': 950,\n",
       " 'edsall': 951,\n",
       " 'education': 952,\n",
       " 'educational': 953,\n",
       " 'edward': 954,\n",
       " 'edwards': 955,\n",
       " 'effect': 956,\n",
       " 'effectiveness': 957,\n",
       " 'efficiency': 958,\n",
       " 'effort': 959,\n",
       " 'egla': 960,\n",
       " 'egypt': 961,\n",
       " 'eight': 962,\n",
       " 'either': 963,\n",
       " 'elderly': 964,\n",
       " 'elected': 965,\n",
       " 'election': 966,\n",
       " 'elections': 967,\n",
       " 'electronic': 968,\n",
       " 'elevation': 969,\n",
       " 'eleven': 970,\n",
       " 'ely': 971,\n",
       " 'emerges': 972,\n",
       " 'emissions': 973,\n",
       " 'empire': 974,\n",
       " 'employment': 975,\n",
       " 'empowerment': 976,\n",
       " 'enable': 977,\n",
       " 'encoded': 978,\n",
       " 'encodes': 979,\n",
       " 'encounters': 980,\n",
       " 'encourage': 981,\n",
       " 'end': 982,\n",
       " 'ended': 983,\n",
       " 'endemic': 984,\n",
       " 'ene': 985,\n",
       " 'energy': 986,\n",
       " 'engine': 987,\n",
       " 'engineer': 988,\n",
       " 'england': 989,\n",
       " 'english': 990,\n",
       " 'enter': 991,\n",
       " 'entered': 992,\n",
       " 'entering': 993,\n",
       " 'enterprise': 994,\n",
       " 'enters': 995,\n",
       " 'entertainment': 996,\n",
       " 'entire': 997,\n",
       " 'entitled': 998,\n",
       " 'entrepreneur': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ee488dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the following words might not be present in the dictionary when the notebook is run again\n",
    "#as the sampling of the sentences in done randomly\n",
    "#can check with other words\n",
    "\n",
    "vocab[\"comedy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57143f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1293"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"great\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7778324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocabulary:  3401\n"
     ]
    }
   ],
   "source": [
    "print(\"length of vocabulary: \", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d76527",
   "metadata": {},
   "source": [
    "<b> our vocab has 3401 unique words </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6fbee4",
   "metadata": {},
   "source": [
    "<h2> Defining the Dataset for Neural Network </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0601497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(word): #returns the one hot encoded vector for a given word (given word must be in the vocabulary)\n",
    "    encoding=np.zeros(len(vocab))\n",
    "    encoding[vocab[word]]=1 #setting the index corresponding to the word as 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cd73b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_words(word_position,word_list,window=2): #return the context word lsit for a given word position in a sentence\n",
    "    \n",
    "    lower_index=max(word_position-window,0)\n",
    "    upper_index=min(word_position+window,len(word_list)-1)\n",
    "    \n",
    "    context_words=[] #list of context words\n",
    "    for i in range(lower_index,upper_index+1):\n",
    "        if i==word_position: #index of the word itself\n",
    "            continue\n",
    "        else: #context word index\n",
    "            context_words.append(word_list[i])\n",
    "                \n",
    "    return context_words #returns list of context words for a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04e2b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating x and y for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f05f471f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 819.60it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train=[] #list of words\n",
    "y_train=[] #list of context words\n",
    "\n",
    "for sentence in tqdm(preprocessed_data):\n",
    "    word_list=sentence.split() #getting the list of words in the sentence\n",
    "    for i in range(len(word_list)): #for all words in a sentence\n",
    "        x_word=one_hot(word_list[i])\n",
    "        context_words=get_context_words(i,word_list)\n",
    "        for word in context_words: #for each context word for a word\n",
    "            y_word=one_hot(word)\n",
    "            #adding the x_word and y_word to the main dataset\n",
    "            x_train.append(x_word)\n",
    "            y_train.append(y_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11a3e008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33788"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3aa1f9f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33788"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c599ee",
   "metadata": {},
   "source": [
    "<b> Now we have 33,788 word-context words pair using which we train the neural network </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51383046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0ab9bb4",
   "metadata": {},
   "source": [
    "<h2> Defining the Neural Network </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a246c4",
   "metadata": {},
   "source": [
    "<b> W2V DENSE VECTOR SIZE=30--> NEURONS IN HIDDEN LAYER=30 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db23e81",
   "metadata": {},
   "source": [
    "<b> VOCABULARY SIZE 3401--> NEURONS IN INPUT AND OUTPUT LAYER=3401 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "850339e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, Dense, Flatten, Input\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(30, activation='relu',use_bias=True,kernel_initializer=\"glorot_uniform\"\n",
    "                ,bias_initializer=RandomNormal(mean=0.0, stddev=0.05),input_dim=len(vocab))) #hidden layer\n",
    "#input size has been defined as the size of the vocabulary\n",
    "          \n",
    "model.add(Dense(len(vocab), activation=softmax,use_bias=True,kernel_initializer=\"glorot_uniform\"\n",
    "                ,bias_initializer=RandomNormal(mean=0.0, stddev=0.05))) #output layer- softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473d477",
   "metadata": {},
   "source": [
    "<h2> Compiling the Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eab15752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy' #cross entropy as the loss function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bcc55af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                102060    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3401)              105431    \n",
      "=================================================================\n",
      "Total params: 207,491\n",
      "Trainable params: 207,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3eac16",
   "metadata": {},
   "source": [
    "<h2> Training the Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe1578a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "528/528 [==============================] - 6s 3ms/step - loss: 7.2238\n",
      "Epoch 2/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.6677\n",
      "Epoch 3/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.6402\n",
      "Epoch 4/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.6264\n",
      "Epoch 5/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.6155\n",
      "Epoch 6/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.6049\n",
      "Epoch 7/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.5917\n",
      "Epoch 8/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.5758\n",
      "Epoch 9/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.5567\n",
      "Epoch 10/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.5340\n",
      "Epoch 11/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.5064\n",
      "Epoch 12/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.4745\n",
      "Epoch 13/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.4369\n",
      "Epoch 14/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.3940\n",
      "Epoch 15/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.3457\n",
      "Epoch 16/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.2925\n",
      "Epoch 17/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.2345\n",
      "Epoch 18/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.1720\n",
      "Epoch 19/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.1063\n",
      "Epoch 20/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 6.0382\n",
      "Epoch 21/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.9679\n",
      "Epoch 22/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.8963\n",
      "Epoch 23/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.8238\n",
      "Epoch 24/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.7505\n",
      "Epoch 25/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.6777\n",
      "Epoch 26/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.6048\n",
      "Epoch 27/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.5332\n",
      "Epoch 28/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.4625\n",
      "Epoch 29/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.3933\n",
      "Epoch 30/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.3258\n",
      "Epoch 31/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.2603\n",
      "Epoch 32/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.1968\n",
      "Epoch 33/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.1358\n",
      "Epoch 34/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.0771\n",
      "Epoch 35/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 5.0212\n",
      "Epoch 36/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.9672\n",
      "Epoch 37/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.9157\n",
      "Epoch 38/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.8660\n",
      "Epoch 39/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.8189\n",
      "Epoch 40/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.7734\n",
      "Epoch 41/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.7300\n",
      "Epoch 42/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.6887\n",
      "Epoch 43/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.6495\n",
      "Epoch 44/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.6122\n",
      "Epoch 45/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.5757\n",
      "Epoch 46/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.5415\n",
      "Epoch 47/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.5088\n",
      "Epoch 48/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.4774\n",
      "Epoch 49/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.4481\n",
      "Epoch 50/50\n",
      "528/528 [==============================] - 2s 3ms/step - loss: 4.4192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x205b909a880>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=64\n",
    "epochs=50\n",
    "\n",
    "#train the model\n",
    "model.fit(x=np.array(x_train),y=np.array(y_train),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs)\n",
    "                    #callbacks=cp_callback) # Pass callback to training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137306ab",
   "metadata": {},
   "source": [
    "<b> the training loss has improved from 7.2238 to 4.41 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02634917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1074176",
   "metadata": {},
   "source": [
    "<h2> Analysing the Weight Matrix </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b282672e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3401"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab) #vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b1b5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights() # returs a numpy list of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be88a52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2edf1988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3401, 30)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0].shape #Wword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11d7a625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1].shape #hidden layer biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e4fdacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 3401)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[2].shape #Wcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5a23b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3401,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[3].shape #output layer biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3e259",
   "metadata": {},
   "source": [
    "<h2> Getting Word Embeddings for any word </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c971d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_word=weights[0]\n",
    "bias=weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca8a746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word=\"great\" #enter the word here\n",
    "encoding=one_hot(word) #one hot encoding for the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "872c0c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=np.matmul(encoding,w_word)+bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4fa47324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11897421,  1.04536515,  1.03667539,  1.27910376,  0.95242138,\n",
       "        0.87302552,  0.36116925, -0.00405413,  0.49722481, -0.09004649,\n",
       "       -0.0826215 , -0.05988851, -0.01450284, -0.11034647,  0.84957301,\n",
       "        0.40098637,  1.25051621,  2.08496851, -0.05835783,  0.71212136,\n",
       "       -0.13719042,  1.18773946,  0.49917839,  1.56662285,  1.11882298,\n",
       "        1.09841652,  1.22299717,  1.16509277,  0.5704588 ,  0.50117218])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding #30-dimensional dense embedding for the word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08986bdb",
   "metadata": {},
   "source": [
    "<h1> ---Built the W2V Model--- </h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
